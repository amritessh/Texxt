{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 post                 tags\n",
      "0   I  have received a message that I have won a l...         lottery scam\n",
      "1   I got an email that I won a lottery is it a scam?         lottery scam\n",
      "2    I got a  call from unknown number and have wo...         lottery scam\n",
      "3     I have deposited money to get the lottery money         lottery scam\n",
      "4                      how can I report lottery fraud         lottery scam\n",
      "5                how can I file FIR for lottery fraud         lottery scam\n",
      "6   I have transferred money to claim prize money ...         lottery scam\n",
      "7    how to make a complaint for online lottery fraud         lottery scam\n",
      "8   I have paid processing charges for lottery how...         lottery scam\n",
      "9   I have paid transfer charges for lottery how c...         lottery scam\n",
      "10           my facebook id is hacked , what can I do  social media crimes\n",
      "11               I got a message saying I won a prize         lottery scam\n",
      "12  I got a notification that I have won a lottery...         lottery scam\n",
      "13                                  is lottery a scam         lottery scam\n",
      "14                          is lottery legal in india         lottery scam\n",
      "15                          what do the scammers want         lottery scam\n",
      "16   I got a message saying I won a prize/ did I win?         lottery scam\n",
      "17                       can I report a lottery fraud         lottery scam\n",
      "18        I paid money for lottery can I get it back?         lottery scam\n",
      "19            can I get compensation for lottery scam         lottery scam\n",
      "1421\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\HP\\Downloads\\crimedatasetcsv (1).csv\")\n",
    "df = df[pd.notnull(df['tags'])]\n",
    "print(df.head(20))\n",
    "print(df['post'].apply(lambda x: len(x.split(' '))).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_tags = ['lottery scam','loan fraud','job scam','phishing','social media crimes']\n",
    "plt.figure(figsize=(10,5))\n",
    "df.tags.value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I got a  call from unknown number and have won a prize in a lottery \n",
      "Tag: lottery scam\n"
     ]
    }
   ],
   "source": [
    "def print_plot(index):\n",
    "    example = df[df.index == index][['post', 'tags']].values[0]\n",
    "    if len(example) > 0:\n",
    "        print(example[0])\n",
    "        print('Tag:', example[1])\n",
    "print_plot(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook id hacked\n",
      "Tag: social media crimes\n"
     ]
    }
   ],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text\n",
    "    \n",
    "df['post'] = df['post'].apply(clean_text)\n",
    "print_plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.post\n",
    "y = df.tags\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8775510204081632\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "       lottery scam       0.67      1.00      0.80         8\n",
      "         loan fraud       1.00      0.40      0.57         5\n",
      "           job scam       1.00      0.82      0.90        11\n",
      "           phishing       0.91      0.95      0.93        22\n",
      "social media crimes       1.00      1.00      1.00         3\n",
      "\n",
      "          micro avg       0.88      0.88      0.88        49\n",
      "          macro avg       0.92      0.83      0.84        49\n",
      "       weighted avg       0.91      0.88      0.87        49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9387755102040817\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "       lottery scam       1.00      1.00      1.00         8\n",
      "         loan fraud       1.00      0.80      0.89         5\n",
      "           job scam       0.91      0.91      0.91        11\n",
      "           phishing       0.91      0.95      0.93        22\n",
      "social media crimes       1.00      1.00      1.00         3\n",
      "\n",
      "          micro avg       0.94      0.94      0.94        49\n",
      "          macro avg       0.96      0.93      0.95        49\n",
      "       weighted avg       0.94      0.94      0.94        49\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)\n",
    "y_pred = sgd.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8979591836734694\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "       lottery scam       1.00      1.00      1.00         8\n",
      "         loan fraud       1.00      0.80      0.89         5\n",
      "           job scam       1.00      0.91      0.95        11\n",
      "           phishing       0.95      0.86      0.90        22\n",
      "social media crimes       0.43      1.00      0.60         3\n",
      "\n",
      "          micro avg       0.90      0.90      0.90        49\n",
      "          macro avg       0.88      0.91      0.87        49\n",
      "       weighted avg       0.94      0.90      0.91        49\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"f:\\GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Memorial_Hospital',\n",
       " 'Seniors',\n",
       " 'memorandum',\n",
       " 'elephant',\n",
       " 'Trump',\n",
       " 'Census',\n",
       " 'pilgrims',\n",
       " 'De',\n",
       " 'Dogs',\n",
       " '###-####_ext',\n",
       " 'chaotic',\n",
       " 'forgive',\n",
       " 'scholar',\n",
       " 'Lottery',\n",
       " 'decreasing',\n",
       " 'Supervisor',\n",
       " 'fundamentally',\n",
       " 'Fitness',\n",
       " 'abundance',\n",
       " 'Hold']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "list(islice(wv.vocab, 13030, 13050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  \n",
      "WARNING:root:cannot compute similarity with no input []\n"
     ]
    }
   ],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "    \n",
    "train, test = train_test_split(df, test_size=0.3, random_state = 42)\n",
    "\n",
    "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['post']), axis=1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['post']), axis=1).values\n",
    "\n",
    "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "import gensim \n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "\n",
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the post.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(TaggedDocument(v.split(), [label]))\n",
    "    return labeled\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.post, df.tags, random_state=0, test_size=0.3)\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['requesting', 'username', 'password', 'credit', 'card', 'debit', 'card', 'personal', 'information', 'phishing'], tags=['Train_0']),\n",
       " TaggedDocument(words=['identify', 'job', 'fraud'], tags=['Train_1'])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<00:00, 8473.55it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<00:00, 108166.42it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<00:00, 84791.93it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<00:00, 13373.26it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<00:00, 11645.02it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<00:00, 41699.58it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<00:00, 1965.44it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<?, ?it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<?, ?it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<?, ?it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<00:00, 12115.96it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<00:00, 46223.76it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<?, ?it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<00:00, 68514.91it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<?, ?it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<00:00, 45418.55it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<?, ?it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<00:00, 10316.12it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<?, ?it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<00:00, 6545.66it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<?, ?it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<00:00, 161050.07it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<00:00, 161088.49it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<?, ?it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<00:00, 161319.38it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<?, ?it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<?, ?it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<00:00, 140071.14it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<?, ?it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<?, ?it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<00:00, 4432.04it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors\n",
    "    \n",
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5306122448979592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           job scam       0.54      0.70      0.61        10\n",
      "         loan fraud       0.00      0.00      0.00         5\n",
      "       lottery scam       0.33      0.25      0.29         8\n",
      "           phishing       0.54      0.65      0.59        20\n",
      "social media crimes       0.67      0.67      0.67         6\n",
      "\n",
      "          micro avg       0.53      0.53      0.53        49\n",
      "          macro avg       0.42      0.45      0.43        49\n",
      "       weighted avg       0.47      0.53      0.49        49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(train_vectors_dbow, y_train)\n",
    "logreg = logreg.fit(train_vectors_dbow, y_train)\n",
    "y_pred = logreg.predict(test_vectors_dbow)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples, validate on 12 samples\n",
      "Epoch 1/2\n",
      "100/100 [==============================] - ETA: 6s - loss: 1.6450 - accuracy: 0.15 - ETA: 1s - loss: 1.6285 - accuracy: 0.17 - 3s 32ms/step - loss: 1.6120 - accuracy: 0.2000 - val_loss: 1.5247 - val_accuracy: 0.6667\n",
      "Epoch 2/2\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.5583 - accuracy: 0.31 - 0s 539us/step - loss: 1.5146 - accuracy: 0.4700 - val_loss: 1.5117 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "\n",
    "train_size = int(len(df) * .7)\n",
    "train_posts = df['post'][:train_size]\n",
    "train_tags = df['tags'][:train_size]\n",
    "\n",
    "test_posts = df['post'][train_size:]\n",
    "test_tags = df['tags'][train_size:]\n",
    "\n",
    "max_words = 1000\n",
    "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
    "tokenize.fit_on_texts(train_posts) # only fit on train\n",
    "\n",
    "x_train = tokenize.texts_to_matrix(train_posts)\n",
    "x_test = tokenize.texts_to_matrix(test_posts)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA:  - 0s 241us/step\n",
      "Test accuracy: 0.5918367505073547\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('crime.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
